# Diffusion Refinement for Medical Image Segmentation

åŸºäºæ‰©æ•£ä¼˜åŒ–çš„åŒ»å­¦å›¾åƒæ™ºèƒ½åˆ†å‰²ç³»ç»Ÿ / Medical Image Intelligent Segmentation System with Diffusion Refinement

## ğŸ¯ Overview

This system combines deep learning segmentation models with diffusion models to achieve high-precision automatic segmentation of target regions in medical images. The diffusion denoising mechanism further optimizes the quality of segmentation boundaries.

æœ¬ç³»ç»Ÿåˆ©ç”¨æ·±åº¦å­¦ä¹ åˆ†å‰²æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œå®ç°å¯¹åŒ»å­¦å›¾åƒä¸­ç›®æ ‡åŒºåŸŸçš„é«˜ç²¾åº¦è‡ªåŠ¨åˆ†å‰²ï¼Œå¹¶é€šè¿‡æ‰©æ•£å»å™ªæœºåˆ¶è¿›ä¸€æ­¥ä¼˜åŒ–åˆ†å‰²è¾¹ç•Œè´¨é‡ã€‚

## âœ¨ Features

- ğŸ¥ **Medical Image Support**: CT, endoscopic images, X-ray, and more
- ğŸ“· **Flexible Input**: Single image or batch processing
- ğŸ”¬ **Two-Stage Segmentation**: Coarse segmentation + Diffusion refinement
- ğŸ¨ **Interactive Visualization**: Web-based interface with real-time results
- ğŸ“Š **Multiple Formats**: DICOM, NIfTI, PNG, JPEG, TIFF, and more

## ğŸ—ï¸ System Architecture

```
Image Input â†’ Preprocessing â†’ CPUNet (Coarse Segmentation)
                                    â†“
          Result Visualization â† Postprocessing â† Diffusion Refinement
```

### Components

1. **Preprocessing Module** (`utils/preprocessing.py`)
   - Image loading (DICOM, NIfTI, common formats)
   - Intensity normalization
   - Contrast enhancement (CLAHE)
   - Denoising

2. **CPUNet** (`models/cpunet.py`)
   - U-Net based encoder-decoder architecture
   - Attention gates for feature focusing
   - Multi-scale feature extraction
   - Produces initial coarse segmentation

3. **Diffusion Refinement** (`models/diffusion.py`)
   - Denoising Diffusion Probabilistic Model (DDPM)
   - Conditioned on original image
   - Iterative boundary refinement
   - Noise-based uncertainty modeling

4. **Postprocessing Module** (`utils/postprocessing.py`)
   - Morphological operations (opening, closing)
   - Small object removal
   - Hole filling
   - Boundary smoothing

5. **Visualization Module** (`utils/visualization.py`)
   - Overlay creation
   - Contour drawing
   - Comparison visualization
   - Result export

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/Diffusinorefinement.git
cd Diffusinorefinement

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
.\venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

## ğŸš€ Quick Start

### Web Interface

Launch the Gradio-based web interface:

```bash
python app.py
```

Then open your browser to `http://localhost:7860`

### Python API

```python
from pipeline import SegmentationPipeline

# Initialize the pipeline
pipeline = SegmentationPipeline(
    cpunet_weights="checkpoints/cpunet_best.pth",      # Optional
    diffusion_weights="checkpoints/diffusion_best.pth", # Optional
    device="auto",
    use_diffusion=True,
)

# Segment a single image
mask = pipeline.segment_image("path/to/medical_image.png")

# Get intermediate results
results = pipeline.segment_image(
    "path/to/image.png",
    return_intermediate=True
)
# results contains: input, coarse_mask, refined_mask, original_size

# Segment batch of images
masks = pipeline.segment_batch([
    "image1.png",
    "image2.png",
    "image3.png"
])

# Visualize results
pipeline.visualize("path/to/image.png", save_path="result.png")
```

### Command Line

```bash
# Segment a single image
python -c "
from pipeline import SegmentationPipeline
pipeline = SegmentationPipeline()
mask = pipeline.segment_image('input.png')
from PIL import Image
import numpy as np
Image.fromarray((mask * 255).astype(np.uint8)).save('output_mask.png')
"
```

## ğŸ“ Training

### Data Preparation

Organize your data in the following structure:

```
data/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ image_001.png
â”‚   â”œâ”€â”€ image_002.png
â”‚   â””â”€â”€ ...
â””â”€â”€ masks/
    â”œâ”€â”€ image_001.png
    â”œâ”€â”€ image_002.png
    â””â”€â”€ ...
```

### Train Models

```bash
# Train both CPUNet and Diffusion model
python train.py --model both --data_dir ./data --output_dir ./checkpoints --epochs 100

# Train only CPUNet
python train.py --model cpunet --data_dir ./data --epochs 100

# Train only Diffusion model (requires pretrained CPUNet)
python train.py --model diffusion --data_dir ./data --cpunet_weights ./checkpoints/cpunet_best.pth
```

### Training Options

| Option | Description | Default |
|--------|-------------|---------|
| `--model` | Model to train (cpunet/diffusion/both) | both |
| `--data_dir` | Data directory | ./data |
| `--output_dir` | Output directory | ./checkpoints |
| `--epochs` | Number of epochs | 100 |
| `--batch_size` | Batch size | 8 |
| `--lr` | Learning rate | 1e-4 |
| `--device` | Device (auto/cuda/cpu) | auto |

## ğŸ“ Project Structure

```
Diffusinorefinement/
â”œâ”€â”€ app.py                 # Gradio web interface
â”œâ”€â”€ pipeline.py            # Main segmentation pipeline
â”œâ”€â”€ train.py               # Training script
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ config.py          # Configuration settings
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cpunet.py          # CPUNet model
â”‚   â””â”€â”€ diffusion.py       # Diffusion refinement model
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessing.py   # Image preprocessing
â”‚   â”œâ”€â”€ postprocessing.py  # Mask postprocessing
â”‚   â””â”€â”€ visualization.py   # Result visualization
â”œâ”€â”€ data/                  # Training data (not included)
â””â”€â”€ checkpoints/           # Saved models (not included)
```

## ğŸ”§ Configuration

Edit `configs/config.py` to customize:

```python
# Model Configuration
MODEL_CONFIG = {
    "cpunet": {
        "in_channels": 1,      # Grayscale input
        "out_channels": 1,     # Binary segmentation
        "base_channels": 64,   # Base feature channels
        "num_blocks": 4,       # Encoder/decoder depth
    },
    "diffusion": {
        "num_timesteps": 1000, # Diffusion timesteps
        "beta_start": 0.0001,  # Noise schedule start
        "beta_end": 0.02,      # Noise schedule end
        "channels": 64,        # Model channels
    }
}

# Image Configuration
IMAGE_CONFIG = {
    "input_size": (256, 256),  # Model input size
}
```

## ğŸ“Š Supported Image Formats

| Format | Extension | Description |
|--------|-----------|-------------|
| DICOM | .dcm | Medical imaging standard |
| NIfTI | .nii, .nii.gz | Neuroimaging format |
| PNG | .png | Lossless compression |
| JPEG | .jpg, .jpeg | Lossy compression |
| TIFF | .tif, .tiff | High quality images |
| BMP | .bmp | Bitmap images |

## ğŸ”¬ Technical Details

### CPUNet Architecture

- Encoder: 4 downsampling blocks with 64â†’128â†’256â†’512 channels
- Decoder: 4 upsampling blocks with skip connections
- Attention gates for feature refinement
- Output: Sigmoid activated probability map

### Diffusion Model

- Based on DDPM (Denoising Diffusion Probabilistic Models)
- 1000 timesteps with linear beta schedule
- Conditioned on original image for context
- U-Net denoiser with timestep embedding

### Refinement Process

1. Add controlled noise to coarse mask
2. Iteratively denoise conditioned on image
3. Noise level determines refinement strength
4. Result: smoother, more accurate boundaries

## ğŸ“ Citation

If you use this code in your research, please cite:

```bibtex
@software{diffusion_refinement_segmentation,
  title = {Diffusion Refinement for Medical Image Segmentation},
  year = {2024},
  url = {https://github.com/yourusername/Diffusinorefinement}
}
```

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- U-Net architecture from Ronneberger et al.
- DDPM from Ho et al.
- Attention U-Net from Oktay et al.